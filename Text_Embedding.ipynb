{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvGOM0sj2uPGvrCTx1xVdU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujialagar/youtube-streaming-data/blob/main/Text_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "!pip install tensorflow-text\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.spatial.distance import jaccard"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23qcG8rlrQGZ",
        "outputId": "caaa05f7-c0a7-4a6b-8306-0f33e0bc1e41"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text) (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Jaccard similarity\n",
        "sentence_1 = \"the oceans is very dense to see\"\n",
        "sentence_2 = \"gorgeous to see oceans and the vast expanse of the ocean is mesmerizing to behold\"\n",
        "sentences_3 = \"The deep blue sea holds many mysteries waiting to be discovered also it has crystal clear waters make the ocean a beautiful sight.\"\n",
        "sentences_4 = \"Exploring the underwater world reveals stunning coral reefs when watching the sun rise over the ocean is a serene experience.\"\n",
        "sentences_5 = \"The ocean's waves create a rhythmic and calming sound also the oceans vastness is both humbling and awe-inspiring.\"\n",
        "\n",
        "# Tokenize and create sets of words\n",
        "set_1 = set(sentence_1.lower().split())\n",
        "set_2 = set(sentence_2.lower().split())\n",
        "set_3 = set(sentences_3.lower().split())\n",
        "set_4 = set(sentences_4.lower().split())\n",
        "set_5 = set(sentences_5.lower().split())\n",
        "\n",
        "\n",
        "intersection = set_1.intersection(set_2)\n",
        "union = set_1.union(set_2)\n",
        "jaccard_similarity = len(intersection) / len(union)\n",
        "\n",
        "\n",
        "print(f\"Jaccard Similarity: {jaccard_similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Af7dFMrRsd68",
        "outputId": "81c05a66-49c3-4099-c1b0-ab07b4b1a8e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity: 0.3333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Euclidean distance\n",
        "\n",
        "import math\n",
        "\n",
        "# Tokenize sentences\n",
        "tokens_1 = sentence_1.lower().split()\n",
        "tokens_2 = sentence_2.lower().split()\n",
        "tokens_3 = sentences_3.lower().split()\n",
        "tokens_4 = sentences_4.lower().split()\n",
        "tokens_5 = sentences_5.lower().split()\n",
        "\n",
        "# Create a combined vocabulary\n",
        "vocabulary = list(set(tokens_1) | set(tokens_2))\n",
        "vocabulary.sort()\n",
        "\n",
        "# Create BoW vectors\n",
        "bow_1 = [tokens_1.count(word) for word in vocabulary]\n",
        "bow_2 = [tokens_2.count(word) for word in vocabulary]\n",
        "bow_3 = [tokens_3.count(word) for word in vocabulary]\n",
        "bow_4 = [tokens_4.count(word) for word in vocabulary]\n",
        "bow_5 = [tokens_5.count(word) for word in vocabulary]\n",
        "\n",
        "\n",
        "euclidean_distance = math.sqrt(sum((bow_1[i] - bow_2[i]) ** 2 for i in range(len(vocabulary))))\n",
        "\n",
        "\n",
        "print(f\"Euclidean Distance: {euclidean_distance}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVMEDbYdd1R3",
        "outputId": "548aeeaf-3dff-41a4-beba-ba92afd27e70"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distance: 3.4641016151377544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# BoW vectors for each sentence\n",
        "print(f\"BoW Vector for Sentence 1: {bow_1}\")\n",
        "print(f\"BoW Vector for Sentence 2: {bow_2}\")\n",
        "print(f\"BoW Vector for Sentence 3: {bow_3}\")\n",
        "print(f\"BoW Vector for Sentence 4: {bow_4}\")\n",
        "print(f\"BoW Vector for Sentence 5: {bow_5}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epXt1VH3eOFS",
        "outputId": "f582a2d9-a361-4c42-904d-5a01fa14f23d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW Vector for Sentence 1: [0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1]\n",
            "BoW Vector for Sentence 2: [1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 0]\n",
            "BoW Vector for Sentence 3: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 0, 0]\n",
            "BoW Vector for Sentence 4: [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 0]\n",
            "BoW Vector for Sentence 5: [2, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate sentence vectors as the average of word vectors (BoW vectors in this case)\n",
        "cbow_vector_1 = [sum(bow_1) / len(bow_1)] * len(vocabulary)\n",
        "cbow_vector_2 = [sum(bow_2) / len(bow_2)] * len(vocabulary)\n",
        "cbow_vector_3 = [sum(bow_3) / len(bow_3)] * len(vocabulary)\n",
        "cbow_vector_4 = [sum(bow_4) / len(bow_4)] * len(vocabulary)\n",
        "cbow_vector_5 = [sum(bow_5) / len(bow_5)] * len(vocabulary)\n",
        "\n",
        "\n",
        "print(f\"CBOW Vector for Sentence 1: {cbow_vector_1}\")\n",
        "print(f\"CBOW Vector for Sentence 2: {cbow_vector_2}\")\n",
        "print(f\"CBOW Vector for Sentence 3: {cbow_vector_3}\")\n",
        "print(f\"CBOW Vector for Sentence 4: {cbow_vector_4}\")\n",
        "print(f\"CBOW Vector for Sentence 5: {cbow_vector_5}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryBFpSLMeQuD",
        "outputId": "cc752e38-6628-43e9-91ff-ef5ee1c3f694"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW Vector for Sentence 1: [0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667, 0.4666666666666667]\n",
            "CBOW Vector for Sentence 2: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "CBOW Vector for Sentence 3: [0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666, 0.26666666666666666]\n",
            "CBOW Vector for Sentence 4: [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n",
            "CBOW Vector for Sentence 5: [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate dot product\n",
        "dot_product = sum(bow_1[i] * bow_2[i] for i in range(len(vocabulary)))\n",
        "print(f\"Dot Product: {dot_product}\")\n",
        "\n",
        "\n",
        "# Calculate magnitudes\n",
        "magnitude_1 = math.sqrt(sum(bow_1[i] ** 2 for i in range(len(vocabulary))))\n",
        "magnitude_2 = math.sqrt(sum(bow_2[i] ** 2 for i in range(len(vocabulary))))\n",
        "magnitude_3 = math.sqrt(sum(bow_3[i] ** 2 for i in range(len(vocabulary))))\n",
        "magnitude_4 = math.sqrt(sum(bow_4[i] ** 2 for i in range(len(vocabulary))))\n",
        "magnitude_5 = math.sqrt(sum(bow_5[i] ** 2 for i in range(len(vocabulary))))\n",
        "\n",
        "\n",
        "# Calculate Cosine similarity\n",
        "cosine_similarity = dot_product / (magnitude_1 * magnitude_2)\n",
        "cosine_similarity_3 = dot_product / (magnitude_1 * magnitude_3)\n",
        "cosine_similarity_4 = dot_product / (magnitude_1 * magnitude_4)\n",
        "cosine_similarity_5 = dot_product / (magnitude_1 * magnitude_5)\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_similarity}\")\n",
        "print(f\"Cosine Similarity: {cosine_similarity_3}\")\n",
        "print(f\"Cosine Similarity: {cosine_similarity_4}\")\n",
        "print(f\"Cosine Similarity: {cosine_similarity_5}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YNPz6pDeTFP",
        "outputId": "1a0a8953-1fe0-4930-b884-5bdb49509bb7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dot Product: 7\n",
            "Cosine Similarity: 0.6069769786668838\n",
            "Cosine Similarity: 1.0801234497346432\n",
            "Cosine Similarity: 0.7977240352174656\n",
            "Cosine Similarity: 0.8366600265340756\n"
          ]
        }
      ]
    }
  ]
}